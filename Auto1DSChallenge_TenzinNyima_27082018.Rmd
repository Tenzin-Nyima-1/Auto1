---
title: "Auto1 Data Challenge"
author: "Tenzin Nyima"
date: "27 August 2018"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  html_notebook:
    theme: lumen
    toc: yes
    toc_depth: 3
---

__Aim__: to build a statistical model that best predicts the price of cars that's in the best interest of Auto1's business. 

### Question 1 (10 Points)

List as many use cases for the dataset as possible.
_Dataset “Auto1-DS-TestData.csv”: [available here](https://archive.ics.uci.edu/ml/datasets/Automobile)_

__Answer 1.__ Various use cases for the dataset are as follows:

- predicting price as a function of the most relevant car features;

- keeping in consideration of the most informative positively/negatively correlated features while buying/selling a particular car, for example: predicting engine power (horsepower); mileage (city.mpg, highway.mpg); fuel type and cost-effectiveness; number of doors (height, width, and length); correlation of car dimensions to insurance; and the data-driven evidence for the correlation between car brand and price.

### Question 2 (10 Points)

Auto1 has a similar dataset (yet much larger...)
Pick one of the use cases you listed in question 1 and describe how building a statistical
model based on the dataset could best be used to improve Auto1’s business.

__Answer 2.__ For this assignment, I'll identify a model with the best predictors of car price.  Auto1 could use this model as the estimators of car price while buying or selling cars.

### Question 3 (20 Points)

Implement the model you described in question 2 in R or Python. The code has to retrieve
the data, train and test a statistical model, and report relevant performance criteria.

__Answer 3.__ The following R script answers to question 3 in six sections. Each line of the script is followed by a comment that describes the intentions. 

_(Note: this script was ran using R version 3.5.0 (2018-04-23))_

```{r message=FALSE}
library(caret)
library(psych)
library(corrplot)
library(pander)
```

### I. Data Handling

```{r message=FALSE}
# 1. Read the original data;
setwd("~/Documents/Auto1_DataChallenge/Data")
Auto1Data <- read.csv("Auto1-DS-TestData.csv", stringsAsFactors = FALSE)
dim(Auto1Data)
# 1a. Data structure;
pander(summary(Auto1Data))
str(Auto1Data)
```

 
```{r}
# 2. identify NAs;
Auto1Data[Auto1Data == "?"] <- NA
Auto1Data[Auto1Data == ""] <- NA

# 2a. omitting NAs;
# Reason: as a first run of the analysis with a full data;
Auto1Data <- na.omit(Auto1Data)
dim(Auto1Data)
```

```{r}
# 3. Removing uninteresting/NA cols;
NACols  <- c("engine.location","fuel.system","engine.type")
# @Reasons: 1st - only one level; 2-3:error "rank-deficient fit";

# 3a. clean S1;
Auto1Data_1 <- Auto1Data[, -(match(NACols, colnames(Auto1Data)))]
dim(Auto1Data_1)

# str(Auto1Data_1)
# summary(Auto1Data_1)

# 3b. Convert numeric cols to numeric;
numCols <- c("symboling", "normalized.losses", "wheel.base", "length", "width", "height", "curb.weight", "engine.size", "bore","stroke", "compression.ratio", "horsepower", "peak.rpm", "city.mpg", "highway.mpg", "price")

for(i in seq(numCols)){
  Auto1Data_1[,numCols[i]] <- as.numeric(Auto1Data_1[,numCols[i]])
  }
# str(Auto1Data_1)
# summary(Auto1Data_1)

# 3c. Convert char cols to char;
charCols <- setdiff(colnames(Auto1Data_1), numCols)

# @crosscheck;
identical(ncol(Auto1Data_1), length(unique(c(numCols, charCols))))
# [1] TRUE;

for(i in seq(charCols)){
  Auto1Data_1[,charCols[i]] <- as.character(Auto1Data_1[,charCols[i]])
}
# str(Auto1Data_1)
# summary(Auto1Data_1)
```

```{r}
# 4. remove values with only one level;
# Note: to avoid errors due to such values;
charsObsCounts <- vector(mode = "list", length = length(charCols))
names(charsObsCounts) <- charCols

# 4a. identify those obs;
#S1: count values per level in each variable;
for (i in seq(charsObsCounts)) {
  charsObsCounts[[i]] <- table(factor(Auto1Data_1[,names(charsObsCounts)[i]], 
                                      levels = unique(Auto1Data_1[,names(charsObsCounts)[i]])))
  }
#S2: in each variable, get index of levels with <=6 values;
charsObsCounts_1 <- lapply(charsObsCounts, function(x) which(x<=6))
#S2: variable names containing levels with <=6 values;
charsObsCounts_2 <- charsObsCounts_1[unlist(lapply(charsObsCounts_1, function(x) length(x) != 0))]
#S3: get indexes of values with <=6 values in each level;
charsObsCounts_2Ind <- vector("list", length = length(charsObsCounts_2))
for (i in seq(length(charsObsCounts_2))) {
  charsObsCounts_2Ind[[i]]<-which(!is.na(match(Auto1Data_1[,names(charsObsCounts_2)[i]], names(charsObsCounts_2[[i]]))))
}
#S4: final data, after removing rows with <=6 values in each level;
Auto1DataFin <- Auto1Data_1[-(unique(unlist(charsObsCounts_2Ind))), ]
dim(Auto1DataFin)
# [1] 124  23;
#S5: convert char cols into factors;
for (i in seq(charCols)) {
  Auto1DataFin[,charCols[i]] <- factor(Auto1DataFin[,charCols[i]], levels = unique(Auto1DataFin[,charCols[i]]))
}
#summary(Auto1DataFin)
#str(Auto1DataFin)
```

```{r}
# 5. Save data separately;

# @Y(targets);
Auto1DataFin_Y <- Auto1DataFin$price
length(Auto1DataFin_Y)

# @X(predictors);
Auto1DataFin_X <- Auto1DataFin[,-match("price", colnames(Auto1DataFin))]
dim(Auto1DataFin_X)
```

### II. Exploratory Analysis

```{r}
# 1. summary statistics of raw data after omitting NAs; 
pander(summary(Auto1Data))
str(Auto1Data)
```

```{r}
# 2. summary statistics of formatted data after omitting NAs; 
pander(summary(Auto1DataFin))
str(Auto1DataFin)
```

```{r}
# 3. check the distribution of raw, cleaned and log transformed prices;
hist(as.numeric(Auto1Data$price))
hist(Auto1DataFin$price)
hist(log(Auto1DataFin$price))
```

```{r}
# 4. correlation between price and the predictors;
Auto1DataFin_Cor <- corr.test(Auto1DataFin[,names(which(sapply(Auto1DataFin, function(x) is.numeric(x))))],
                              method="pearson", adjust="BH")

# @visualize;
corrplot(Auto1DataFin_Cor$r, method = "number", type = "upper", number.cex = .5)
```

### III. Fitting Models

```{r}
# @Y(targets);
# length(Auto1DataFin_Y)
# [1] 124;
# @X(predictors);
# dim(Auto1DataFin_X)
# [1] 124  22;

# 1. To fit models with similar training and test splits, set up the train control object;
Auto1DataFin_TrCont <- trainControl(method = "cv", number = 10, verboseIter = TRUE)
```

```{r}
# 2. Generalized Linear Model;

# 2.1 without preprocessing;
set.seed(72)
Auto1DataFin_GLM <- train(x = Auto1DataFin_X, y = Auto1DataFin_Y, 
                          method = "glm", 
                          trControl = Auto1DataFin_TrCont)
Auto1DataFin_GLM$results$RMSE

# 2.2 preprocessing 1;
set.seed(72)
Auto1DataFin_GLM_pp1 <- train(x = Auto1DataFin_X, y = Auto1DataFin_Y, 
                          method = "glm", 
                          trControl = Auto1DataFin_TrCont,
                          preProcess = c("zv", "center", "scale", "pca"))
Auto1DataFin_GLM_pp1$results$RMSE

# 2.3 preprocessing 2;
set.seed(72)
Auto1DataFin_GLM_pp2 <- train(x = Auto1DataFin_X, y = Auto1DataFin_Y, 
                              method = "glm", 
                              trControl = Auto1DataFin_TrCont,
                              preProcess = c("zv", "center", "scale", "spatialSign"))
Auto1DataFin_GLM_pp2$results$RMSE
```

```{r}
# 3. Random Forest;
RF_Mtry <- data.frame(mtry=1:15)
set.seed(72)
Auto1DataFin_RF <- train(x = Auto1DataFin_X, y = Auto1DataFin_Y, 
                         tuneGrid = RF_Mtry, method = "rf", 
                         trControl = Auto1DataFin_TrCont)
plot(Auto1DataFin_RF)
```

### IV. Model Comparison

```{r}
# 1. Model lists;
Auto1DataFin_ModLS <- list(GM = Auto1DataFin_GLM_pp1, RF = Auto1DataFin_RF)

# 2. Resample ls;
Auto1DataFin_ModLS_reSmp <- resamples(Auto1DataFin_ModLS)
pander(summary(Auto1DataFin_ModLS_reSmp))

# 3. display;
bwplot(Auto1DataFin_ModLS_reSmp, metric = "RMSE", main = "Model comparison")

# 4. NOTE;
# GLM model with preprocess is slightly better than the RF model, therefore the former is chosen;
```

### V. Variable Importance

```{r}
# 1. Detailed summary of the selected model;
pander(summary(Auto1DataFin_GLM_pp1))

# 2. Display Var imp;
Auto1DataFin_GLM_pp1_VImp <- varImp(Auto1DataFin_GLM_pp1, scale=TRUE)
plot(Auto1DataFin_GLM_pp1_VImp, main = "Variable Importance in GLM model, preprocessed(1)")
```

### VI. Save Worksape

```{r}
# save.image("~/Documents/Auto1_DataChallenge/RScript/Auto1DSChallenge_TenzinNyima_27082018.RData")
# load("~/Documents/Auto1_DataChallenge/RScript/Auto1DSChallenge_TenzinNyima_27082018.RData")
```

### Question 4 (60 Points)

__A.__ Explain each and every of your design choices (e.g., preprocessing, model selection,
hyper parameters, evaluation criteria). Compare and contrast your choices with alternative
methodologies.
__B.__ Describe how you would improve the model in Question 3 if you had more time.

__Answer 4.__ I chose caret package because it's a comprehensive and well-known R package for building machine learning models. As a first run of the analysis and to set up a pipeline, I omitted the NA values. Real world dataset usually contains a lot of NA values, therefore, I usually prefer not to delete them. It may include biases in the data.  If time permits, I would use additional preprocessing methods such as median imputation or k-nearest neighbor to avoid computation errors associated with the missing values. I found several problematic features after several runs of the analysis. I removed them because of issues such as the presence of only one value in a level or the rank-deficient fit error. For this assignment, I chose to predict price as a function of the most relevant car features, makes it a regression problem. The price variable has a skewed distribution. Although the logarithmic transformed value is closest to Gaussian distribution, as mentioned before, for the first run, I'm using the untransformed value for the analysis. Hence, I am using GLM that has a robust estimation of error structure. Many of the predictors are strongly correlated, so to avoid collinearity I chose to preprocess using PCA. Also, I excluded values with little/no variance from the analysis since it provides negligible information to the model. Also chose RF model, which is quite a robust model for the first step of the analysis. I used the RMSE metric to evaluate GLM models. Among the three GLM models, I chose the second one (that preprocesses zero variance variables, centering and scaling, and PCA) with the lowest RMSE value. For the RF model, the hyperparameter mtry 3 performed the best. Between GLM and RF, the former performed slightly better.